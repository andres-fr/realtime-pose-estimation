@article{Papandreou2018,
abstract = {We present a box-free bottom-up approach for the tasks of pose estimation and instance segmentation of people in multi-person images using an efficient single-shot model. The proposed PersonLab model tackles both semantic-level reasoning and object-part associations using part-based modeling. Our model employs a convolutional network which learns to detect individual keypoints and predict their relative displacements, allowing us to group keypoints into person pose instances. Further, we propose a part-induced geometric embedding descriptor which allows us to associate semantic person pixels with their corresponding person instance, delivering instance-level person segmentations. Our system is based on a fully-convolutional architecture and allows for efficient inference, with runtime essentially independent of the number of people present in the scene. Trained on COCO data alone, our system achieves COCO test-dev keypoint average precision of 0.665 using single-scale inference and 0.687 using multi-scale inference, significantly outperforming all previous bottom-up pose estimation systems. We are also the first bottom-up method to report competitive results for the person class in the COCO instance segmentation task, achieving a person category average precision of 0.417.},
archivePrefix = {arXiv},
arxivId = {1803.08225},
author = {Papandreou, George and Zhu, Tyler and Chen, Liang-chieh and Gidaris, Spyros and Tompson, Jonathan and Murphy, Kevin},
eprint = {1803.08225},
file = {:home/a9fb1e/Documents/papers/personlab.pdf:pdf},
keywords = {estimation,person detection and pose,segmentation and},
month = {mar},
title = {{PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model}},
url = {http://arxiv.org/abs/1803.08225},
year = {2018}
}
@article{ZhangZhu2019,
abstract = {Existing human pose estimation approaches often only consider how to improve the model generalisation perfor- mance, but putting aside the significant efficiency problem. This leads to the development of heavy models with poor scalability and cost-effectiveness in practical use. In this work, we investigate the under-studied but practically crit- ical pose model efficiency problem. To this end, we present a new Fast Pose Distillation (FPD) model learning strat- egy. Specifically, the FPD trains a lightweight pose neural network architecture capable ofexecuting rapidly with low computational cost. It is achieved by effectively transferring the pose structure knowledge of a strong teacher network. Extensive evaluations demonstrate the advantages of our FPD method over a broad range ofstate-of-the-art pose es- timation approaches in terms ofmodel cost-effectiveness on two standard benchmark datasets, MPII Human Pose and Leeds Sports Pose.},
author = {{Zhang, Zhu}, Ye},
file = {:home/a9fb1e/Documents/papers/Zhang{\_}Fast{\_}Human{\_}Pose{\_}Estimation{\_}CVPR{\_}2019{\_}paper.pdf:pdf},
journal = {CVPR},
pages = {12},
title = {{Fast Human pose estimation}},
url = {http://openaccess.thecvf.com/content{\_}CVPR{\_}2019/papers/Zhang{\_}Fast{\_}Human{\_}Pose{\_}Estimation{\_}CVPR{\_}2019{\_}paper.pdf},
year = {2019}
}
@article{Newell2017,
abstract = {We introduce associative embedding, a novel method for supervising convolutional neural networks for the task of detection and grouping. A number of computer vision problems can be framed in this manner including multi-person pose estimation, instance segmentation, and multi-object tracking. Usually the grouping of detections is achieved with multi-stage pipelines, instead we propose an approach that teaches a network to simultaneously output detections and group assignments. This technique can be easily integrated into any state-of-the-art network architecture that produces pixel-wise predictions. We show how to apply this method to multi-person pose estimation and report state-of-the-art performance on the MPII and MS-COCO datasets.},
archivePrefix = {arXiv},
arxivId = {1611.05424},
author = {Newell, Alejandro and Huang, Zhiao and Deng, Jia},
eprint = {1611.05424},
file = {:home/a9fb1e/Documents/papers/newell{\_}assoc{\_}embeddings:},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips 2017},
pages = {2278--2288},
title = {{Associative embedding: End-to-end learning for joint detection and grouping}},
url = {https://arxiv.org/abs/1611.05424},
volume = {2017-Decem},
year = {2017}
}
@article{Moon2019,
abstract = {Multi-person pose estimation from a 2D image is an essential technique for human behavior understanding. In this paper, we propose a human pose refinement network that estimates a refined pose from a tuple of an input image and input pose. The pose refinement was performed mainly through an end-to-end trainable multi-stage architecture in previous methods. However, they are highly dependent on pose estimation models and require careful model design. By contrast, we propose a model-agnostic pose refinement method. According to a recent study, state-of-the-art 2D human pose estimation methods have similar error distributions. We use this error statistics as prior information to generate synthetic poses and use the synthesized poses to train our model. In the testing stage, pose estimation results of any other methods can be input to the proposed method. Moreover, the proposed model does not require code or knowledge about other methods, which allows it to be easily used in the post-processing step. We show that the proposed approach achieves better performance than the conventional multi-stage refinement models and consistently improves the performance of various state-of-the-art pose estimation methods on the commonly used benchmark. The code is available in footnote{\{}url{\{}https://github.com/mks0601/PoseFix RELEASE{\}}.}},
archivePrefix = {arXiv},
arxivId = {1812.03595},
author = {Moon, Gyeongsik and Chang, Ju Yong and Lee, Kyoung Mu},
doi = {10.1109/CVPR.2019.00796},
eprint = {1812.03595},
file = {:home/a9fb1e/Documents/papers/posefix.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {And Body Pose,Deep Learning,Face,Gesture},
pages = {7765--7773},
title = {{Posefix: Model-agnostic general human pose refinement network}},
volume = {2019-June},
year = {2019}
}
@article{Dai2020,
abstract = {In this paper, we focus on the coordinate representation in human pose estimation. While being the standard choice, heatmap based representation has not been systematically investigated. We found that the process of coordinate decoding (i.e. transforming the predicted heatmaps to the coordinates) is surprisingly significant for human pose estimation performance, which nevertheless was not recognised before. In light of the discovered importance, we further probe the design limitations of the standard coordinate decoding method and propose a principled distribution-aware decoding method. Meanwhile, we improve the standard coordinate encoding process (i.e. transforming ground-truth coordinates to heatmaps) by generating accurate heatmap distributions for unbiased model training. Taking them together, we formulate a novel Distribution-Aware coordinate Representation for Keypoint (DARK) method. Serving as a model-agnostic plug-in, DARK significantly improves the performance of a variety of state-of-the-art human pose estimation models. Extensive experiments show that DARK yields the best results on COCO keypoint detection challenge, validating the usefulness and effectiveness of our novel coordinate representation idea. The project page containing more details is at https://ilovepose.github.io/coco},
archivePrefix = {arXiv},
arxivId = {2003.07232},
author = {Dai, Hanbin and Zhou, Liangbo and Zhang, Feng and Zhang, Zhengyu and Hu, Hong and Zhu, Xiatian and Ye, Mao},
eprint = {2003.07232},
file = {:home/a9fb1e/Documents/papers/DarkPose.pdf:pdf},
pages = {1--5},
title = {{Technical Report: DarkPose}},
url = {http://arxiv.org/abs/2003.07232},
year = {2020}
}
@article{Cheng2019,
abstract = {Bottom-up human pose estimation methods have difficulties in predicting the correct pose for small persons due to challenges in scale variation. In this paper, we present HigherHRNet: a novel bottom-up human pose estimation method for learning scale-aware representations using high-resolution feature pyramids. Equipped with multi-resolution supervision for training and multi-resolution aggregation for inference, the proposed approach is able to solve the scale variation challenge in bottom-up multi-person pose estimation and localize keypoints more precisely, especially for small person. The feature pyramid in HigherHRNet consists of feature map outputs from HRNet and upsampled higher-resolution outputs through a transposed convolution. HigherHRNet outperforms the previous best bottom-up method by 2.5{\%} AP for medium person on COCO test-dev, showing its effectiveness in handling scale variation. Furthermore, HigherHRNet achieves new state-of-the-art result on COCO test-dev (70.5{\%} AP) without using refinement or other post-processing techniques, surpassing all existing bottom-up methods. HigherHRNet even surpasses all top-down methods on CrowdPose test (67.6{\%} AP), suggesting its robustness in crowded scene. The code and models are available at https://github.com/HRNet/Higher-HRNet-Human-Pose-Estimation.},
archivePrefix = {arXiv},
arxivId = {1908.10357},
author = {Cheng, Bowen and Xiao, Bin and Wang, Jingdong and Shi, Honghui and Huang, Thomas S. and Zhang, Lei},
eprint = {1908.10357},
file = {:home/a9fb1e/Documents/papers/higher{\_}hrnet.pdf:pdf},
title = {{HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation}},
url = {http://arxiv.org/abs/1908.10357},
year = {2019}
}
@article{XiaoGongLu2019,
abstract = {In this report, we present our multi-person keypoint de- tection system for COCO Keypoint Detection Challenge 2019. It contains three main components, which are multi- person detector, high resolution network (HRNet) for key- point detection and pose refinement network. As the core component, our HRNet starts from a high- resoluiton subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the multi-resolution subnet- works in parallel. We conduct repeated multi-scale fu- sions such that each of the high-to-low resolution repre- sents reveive information from other parallel representa- tions over and over, leading to rich high-resolution rep- resentations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. With an additional pose refinement network, our final sub- mitted result achieves an AP of 78.2 on COCO test-dev set and an AP of 75.5 on COCO test-challenge2019 set respectively. The code and models have been publicly available at https://github.com/leoxiaobin/ deep-high-resolution-net.pytorch},
author = {{Xiao, Gong, Lu}, Wen},
file = {:home/a9fb1e/Documents/papers/ByteDanceHRNet.pdf:pdf},
journal = {Joint COCO and Mapillary Workshop at ICCV 2019},
pages = {3--8},
title = {{Technical Report: ByteDance HRNet}},
url = {http://cocodataset.org/files/keypoints{\_}2019{\_}reports/ByteDanceHRNet.pdf},
year = {2019}
}
@article{Li2019,
abstract = {Existing pose estimation approaches fall into two categories: single-stage and multi-stage methods. While multi-stage methods are seemingly more suited for the task, their performance in current practice is not as good as single-stage methods. This work studies this issue. We argue that the current multi-stage methods' unsatisfactory performance comes from the insufficiency in various design choices. We propose several improvements, including the single-stage module design, cross stage feature aggregation, and coarse-to-fine supervision. The resulting method establishes the new state-of-the-art on both MS COCO and MPII Human Pose dataset, justifying the effectiveness of a multi-stage architecture. The source code is publicly available for further research.},
archivePrefix = {arXiv},
arxivId = {1901.00148},
author = {Li, Wenbo and Wang, Zhicheng and Yin, Binyi and Peng, Qixiang and Du, Yuming and Xiao, Tianzi and Yu, Gang and Lu, Hongtao and Wei, Yichen and Sun, Jian},
eprint = {1901.00148},
file = {:home/a9fb1e/Documents/megvii{\_}basis.pdf:pdf},
title = {{Rethinking on Multi-Stage Networks for Human Pose Estimation}},
url = {http://arxiv.org/abs/1901.00148},
year = {2019}
}
@article{Sun2019,
abstract = {This is an official pytorch implementation of Deep High-Resolution Representation Learning for Human Pose Estimation. In this work, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. The code and models have been publicly available at $\backslash$url{\{}https://github.com/leoxiaobin/deep-high-resolution-net.pytorch{\}}.},
archivePrefix = {arXiv},
arxivId = {1902.09212},
author = {Sun, Ke and Xiao, Bin and Liu, Dong and Wang, Jingdong},
doi = {10.1109/CVPR.2019.00584},
eprint = {1902.09212},
file = {:home/a9fb1e/Documents/papers/hrnet{\_}coco.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {And Body Pose,Face,Gesture},
month = {feb},
pages = {5686--5696},
title = {{Deep High-Resolution Representation Learning for Human Pose Estimation}},
url = {http://arxiv.org/abs/1902.09212},
volume = {2019-June},
year = {2019}
}
@misc{Cai2020,
abstract = {In this paper, we propose a novel method called Residual Steps Network (RSN). RSN aggregates features with the same spatial size (Intra-level features) efficiently to obtain delicate local representa- tions, which retain rich low-level spatial information and result in pre- cise keypoint localization. In addition, we propose an efficient attention mechanism - Pose Refine Machine (PRM) to further refine the keypoint locations. Our approach won the 1st place of COCO Keypoint Chal- lenge 2019 and achieves state-of-the-art results on both COCO and MPII benchmarks, without using extra training data and pretrained model. Our single model achieves 78.6 on COCO test-dev, 93.0 on MPII test dataset. Ensembled models achieve 79.2 on COCO test-dev, 77.1 on COCO test-challenge dataset. The source code is publicly available for further research at https://github.com/caiyuanhao1998/RSN/ Keywords:},
author = {Cai, Yuanhao and Wang, Zhicheng and Luo, Zhengxiong and Yin, Binyi and Du, Angang and Wang, Haoqian and Zhou, Xinyu and Zhou, Erjin and Zhang, Xiangyu and Sun, Jian},
file = {:home/a9fb1e/Documents/papers/rsn{\_}paper.pdf:pdf},
title = {{Learning Delicate Local Representations for Multi-Person Pose Estimation}},
year = {2020}
}
@article{Krahenbuhl2011,
abstract = {Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While regionlevel models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.},
author = {Kr{\"{a}}henb{\"{u}}hl, Philipp and Koltun, Vladlen},
file = {:home/a9fb1e/Documents/papers/densecrf-poster.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011, NIPS 2011},
number = {1},
pages = {2009},
title = {{Efficient inference in fully connected crfs with Gaussian edge potentials}},
volume = {81},
year = {2011}
}
@article{Dai2020a,
abstract = {In this paper, we focus on the coordinate representation in human pose estimation. While being the standard choice, heatmap based representation has not been systematically investigated. We found that the process of coordinate decoding (i.e. transforming the predicted heatmaps to the coordinates) is surprisingly significant for human pose estimation performance, which nevertheless was not recognised before. In light of the discovered importance, we further probe the design limitations of the standard coordinate decoding method and propose a principled distribution-aware decoding method. Meanwhile, we improve the standard coordinate encoding process (i.e. transforming ground-truth coordinates to heatmaps) by generating accurate heatmap distributions for unbiased model training. Taking them together, we formulate a novel Distribution-Aware coordinate Representation for Keypoint (DARK) method. Serving as a model-agnostic plug-in, DARK significantly improves the performance of a variety of state-of-the-art human pose estimation models. Extensive experiments show that DARK yields the best results on COCO keypoint detection challenge, validating the usefulness and effectiveness of our novel coordinate representation idea. The project page containing more details is at https://ilovepose.github.io/coco},
archivePrefix = {arXiv},
arxivId = {2003.07232},
author = {Dai, Hanbin and Zhou, Liangbo and Zhang, Feng and Zhang, Zhengyu and Hu, Hong and Zhu, Xiatian and Ye, Mao},
eprint = {2003.07232},
file = {:home/a9fb1e/Documents/Megvii.pdf:pdf},
pages = {3--8},
title = {{Joint COCO and Mapillary Workshop at ICCV 2019 Keypoint Detection Challenge Track Technical Report: Distribution-Aware Coordinate Representation for Human Pose Estimation}},
url = {http://arxiv.org/abs/2003.07232},
year = {2020}
}
@article{Li2019a,
abstract = {We rethink a well-know bottom-up approach for multi-person pose estimation and propose an improved one. The improved approach surpasses the baseline significantly thanks to (1) an intuitional yet more sensible representation, which we refer to as body parts to encode the connection information between keypoints, (2) an improved stacked hourglass network with attention mechanisms, (3) a novel focal L2 loss which is dedicated to hard keypoint and keypoint association (body part) mining, and (4) a robust greedy keypoint assignment algorithm for grouping the detected keypoints into individual poses. Our approach not only works straightforwardly but also outperforms the baseline by about 15{\%} in average precision and is comparable to the state of the art on the MS-COCO test-dev dataset. The code and pre-trained models are publicly available online.},
archivePrefix = {arXiv},
arxivId = {1911.10529},
author = {Li, Jia and Su, Wen and Wang, Zengfu},
eprint = {1911.10529},
file = {:home/a9fb1e/Documents/simplepose.pdf:pdf},
title = {{Simple Pose: Rethinking and Improving a Bottom-up Approach for Multi-Person Pose Estimation}},
url = {http://arxiv.org/abs/1911.10529},
year = {2019}
}
@article{Chen2017,
abstract = {The topic of multi-person pose estimation has been largely improved recently, especially with the development of convolutional neural network. However, there still exist a lot of challenging cases, such as occluded keypoints, invisible keypoints and complex background, which cannot be well addressed. In this paper, we present a novel network structure called Cascaded Pyramid Network (CPN) which targets to relieve the problem from these "hard" keypoints. More specifically, our algorithm includes two stages: GlobalNet and RefineNet. GlobalNet is a feature pyramid network which can successfully localize the "simple" keypoints like eyes and hands but may fail to precisely recognize the occluded or invisible keypoints. Our RefineNet tries explicitly handling the "hard" keypoints by integrating all levels of feature representations from the GlobalNet together with an online hard keypoint mining loss. In general, to address the multi-person pose estimation problem, a top-down pipeline is adopted to first generate a set of human bounding boxes based on a detector, followed by our CPN for keypoint localization in each human bounding box. Based on the proposed algorithm, we achieve state-of-art results on the COCO keypoint benchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1 on the COCO test-challenge dataset, which is a 19{\%} relative improvement compared with 60.5 from the COCO 2016 keypoint challenge.Code (https://github.com/chenyilun95/tf-cpn.git) and the detection results are publicly available for further research.},
archivePrefix = {arXiv},
arxivId = {1711.07319},
author = {Chen, Yilun and Wang, Zhicheng and Peng, Yuxiang and Zhang, Zhiqiang and Yu, Gang and Sun, Jian},
doi = {10.1109/CVPR.2018.00742},
eprint = {1711.07319},
file = {:home/a9fb1e/Documents/papers/cpn{\_}coco.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
month = {nov},
pages = {7103--7112},
title = {{Cascaded Pyramid Network for Multi-Person Pose Estimation}},
url = {http://arxiv.org/abs/1711.07319},
year = {2017}
}
@article{Mehta2017,
abstract = {Fig. 1. We recover the full global 3D skeleton pose in real-time from a single RGB camera, even wireless capture is possible by streaming from a smartphone (left). It enables applications such as controlling a game character, embodied VR, sport motion analysis and reconstruction of community video (right). Community videos (CC BY) courtesy of Real Madrid C.F. [2016] and RUSFENCING-TV [2017]. We present the first real-time method to capture the full global 3D skeletal pose of a human in a stable, temporally consistent manner using a single RGB camera. Our method combines a new convolutional neural network (CNN) based pose regressor with kinematic skeleton fitting. Our novel fully-convolutional pose formulation regresses 2D and 3D joint positions jointly in real time and does not require tightly cropped input frames. A real-time kinematic skeleton fitting method uses the CNN output to yield temporally stable 3D global pose reconstructions on the basis of a coherent kinematic skeleton. This makes our approach the first monocular RGB method usable in real-time applications such as 3D character control—thus far, the only monocular methods for such applications employed specialized RGB-D cam-eras. Our method's accuracy is quantitatively on par with the best offline 3D monocular RGB pose estimation methods. Our results are qualitatively comparable to, and sometimes better than, results from monocular RGB-D approaches, such as the Kinect. However, we show that our approach is more broadly applicable than RGB-D solutions, i.e., it works for outdoor scenes, community videos, and low quality commodity RGB cameras.},
archivePrefix = {arXiv},
arxivId = {1705.01583},
author = {Mehta, Dushyant and Sridhar, Srinath and Sotnychenko, Oleksandr and Rhodin, Helge and Shafiei, Mohammad Mo-hammad Mohammad Mo-hammad Mohammad Mo-hammad and Seidel, Hans-peter and Casas, Dan and Theobalt, Christian and Shafiei, Mohammad Mo-hammad Mohammad Mo-hammad Mohammad Mo-hammad and Seidel, Hans-peter and Xu, Weipeng},
doi = {10.1145/3072959.3073596},
eprint = {1705.01583},
file = {:home/a9fb1e/Documents/VNect{\_}SIGGRAPH2017.pdf:pdf},
journal = {Tog},
keywords = {@BULLET Computing methodologies → Motion capture,Additional Key Words and Phrases,CCS Concepts,body pose,monocular,real time},
number = {14},
pages = {1--13},
title = {{VNect Real-time 3D Human Pose Estimation with a Single RGB Camera ACM Reference format VNect Real-time 3D Human Po.pdf}},
url = {https://arxiv.org/abs/1705.01583},
volume = {36},
year = {2017}
}
@article{Peng2018,
abstract = {The development of object detection in the era of deep learning, from R-CNN [11], Fast/Faster R-CNN [10, 31] to recent Mask R-CNN [14] and RetinaNet [24], mainly come from novel network, new framework, or loss design. However, mini-batch size, a key factor for the training of deep neural networks, has not been well studied for object detection. In this paper, we propose a Large Mini-Batch Object Detector (MegDet) to enable the training with a large minibatch size up to 256, so that we can effectively utilize at most 128 GPUs to significantly shorten the training time. Technically, we suggest a warmup learning rate policy and Cross-GPU Batch Normalization, which together allow us to successfully train a large mini-batch detector in much less time (e.g., from 33 hours to 4 hours), and achieve even better accuracy. The MegDet is the backbone of our submission (mmAP 52.5{\%}) to COCO 2017 Challenge, where we won the 1st place of Detection task.},
archivePrefix = {arXiv},
arxivId = {1711.07240},
author = {Peng, Chao and Xiao, Tete and Li, Zeming and Jiang, Yuning and Zhang, Xiangyu and Jia, Kai and Yu, Gang and Sun, Jian},
doi = {10.1109/CVPR.2018.00647},
eprint = {1711.07240},
file = {:home/a9fb1e/Documents/megdet.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {6181--6189},
title = {{MegDet: A Large Mini-Batch Object Detector}},
year = {2018}
}
@misc{Hwang2020,
abstract = {We present MoVNect, a lightweight deep neural network to capture 3D human pose using a single RGB camera. To improve the overall performance of the model, we ap- ply the teacher-student learning method based knowledge distillation to 3D human pose estimation. Real-time post- processing makes the CNN output yield temporally stable 3D skeletal information, which can be used in applications directly. We implement a 3D avatar application running on mobile in real-time to demonstrate that our network achieves both high accuracy and fast inference time. Ex- tensive evaluations show the advantages ofour lightweight model with the proposed training method over previous 3D pose estimation methods on the Human3.6M dataset and mobile devices. 1.},
author = {Hwang, Dong-Hyun and Kim, Suntae and Monet, Nicolas},
file = {:home/a9fb1e/Documents/3d{\_}keypose{\_}teacher{\_}student.pdf:pdf},
title = {{Lightweight 3D Human Pose Estimation Network Training Using Teacher-Student Learning}},
url = {https://arxiv.org/abs/2001.05097},
year = {2020}
}
@article{Zhang2019,
abstract = {Human keypoint detection from a single image is very challenging due to occlusion, blur, illumination and scale variance of person instances. In this paper, we find that context information plays an important role in addressing these issues, and propose a novel method named progressive context refinement (PCR) for human keypoint detection. First, we devise a simple but effective context-aware module (CAM) that can efficiently integrate spatial and channel context information to aid feature learning for locating hard keypoints. Then, we construct the PCR model by stacking several CAMs sequentially with shortcuts and employ multi-task learning to progressively refine the context information and predictions. Besides, to maximize PCR's potential for the aforementioned hard case inference, we propose a hard-negative person detection mining strategy together with a joint-training strategy by exploiting the unlabeled coco dataset and external dataset. Extensive experiments on the COCO keypoint detection benchmark demonstrate the superiority of PCR over representative state-of-the-art (SOTA) methods. Our single model achieves comparable performance with the winner of the 2018 COCO Keypoint Detection Challenge. The final ensemble model sets a new SOTA on this benchmark.},
archivePrefix = {arXiv},
arxivId = {1910.12223},
author = {Zhang, Jing and Chen, Zhe and Tao, Dacheng},
eprint = {1910.12223},
file = {:home/a9fb1e/Documents/papers/coco{\_}refinement{\_}sidney.pdf:pdf},
pages = {8--11},
title = {{Human Keypoint Detection by Progressive Context Refinement}},
url = {http://arxiv.org/abs/1910.12223},
year = {2019}
}
